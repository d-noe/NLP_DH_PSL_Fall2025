# Introduction to Natural Language Processing (NLP) â€” DH PSL,  Fall 2025

This repository hosts material for 4x3hours lectures in the context of the *Introduction to Natural Language Processing (NLP)* class from PSL's Master of Digital Humanities, Fall 2025.

1. [Week 1 (29/10)](#week1)
2. [Week 2 (05/11)](#week2)
3. [Week 3 (12/11)](#week3)
4. [Week 4 (19/11)](#week4)

The code and notebooks for the tutorials and hands-on sessions are provided in the [code](./code/) folder. The data used for these sessions is described and stored in [data](./data/).

## Week 1 (29/10): *Modeling Language: Towards Contextualized Representations* <a name="week1"></a>

- Slides: [preview `html`](https://rawcdn.githack.com/d-noe/NLP_DH_PSL_Fall2025/a3dbb3f4a2901602e813b1262d424bcf20b0dcfe/slides/lecture_1_self_contained.html), [`pdf`](./slides/lecture_1.pdf)
- Notebook(s): [BERT Discovery](./code/1_bert_training/Discover_BERT.ipynb), [Word Sense Disambiguation](./code/1_bert_training/Tutorial_1_WSD.ipynb), [Semantic Shifts](./code/1_bert_training/Hands_on_1_SS.ipynb)
- Key notions: n-gram, transformers, self-attention, context, masked language model / causal language model

<details><summary>To go further</summary>

- [(J. Alammar, 2018)](https://jalammar.github.io/illustrated-transformer/): *The Illustrated Transformer* blog post by Jay Alammar.
- [(Ghaseminejad Raeini, 2025)](https://www.sciencedirect.com/science/article/pii/S2949719125000445): *The evolution of language models: From N-Grams to LLMs, and beyond*.
- [(Allen & Hospedales, 2019)](https://proceedings.mlr.press/v97/allen19a.html): *Analogies Explained: Towards Understanding Word Embeddings*.

**Want more hands-on?** Check the [*To go further* section in code folder](./code/README.md#code_supp_1).

</details>

## Week 2 (05/11): *Discovering Structure: Semantic Spaces & Unsupervised Modelling* <a name="week2"></a>

- Slides: 
- Notebook(s): 
- Key notions: cosine similarity, retrieval, clustering
- Main reference(s): 

<details><summary>To go further</summary>

**Topic Modeling**:
- [(Churchill & Singh, 2021)](https://doi.org/10.1145/3507900): *The Evolution of Topic Modeling*.
- [(Li et al., 2024)](https://doi.org/10.1515/dsll-2024-0010): *Applying Topic Modeling to Literary Analysis: A Review*.
- [(Gillings & Hardie, 2022)](https://doi.org/10.1093/llc/fqac075): *The interpretation of topic models for scholarly analysis: An evaluation and critique of current practice*.
- [(Antoniak, 2023)](https://maria-antoniak.github.io/2022/07/27/topic-modeling-for-the-people.html): *Topic Modeling for the People*: an interesting blogpost by Maria Antoniak, sharing a set of steps that you can follow to get coherent topics from most datasets, primarily focusing on LDA. It provides as well many additional references to dig deeper.

**Want more hands-on?** Check the [*To go further* section in code folder](./code/README.md#code_supp_2).

</details>

## Week 3 (12/11): *Inferring Patterns: Supervised Tasks and Adaptation* <a name="week3"></a>

- Slides: 
- Notebook(s): 
- Key notions: classification, fine-tuning
- Main reference(s): 

<details><summary>To go further</summary>
</details>

## Week 4 (19/11):  *LLMs** (TBD) <a name="week4"></a>

- Slides: 
- Notebook(s): 
- Key notions: LLMs
- Main reference(s): 

<details><summary>To go further</summary>

- [(Gallegos et al., 2025)](https://direct.mit.edu/coli/article/50/3/1097/121961/Bias-and-Fairness-in-Large-Language-Models-A): *Bias and Fairness in Large Language Models: A Survey*

</details>
