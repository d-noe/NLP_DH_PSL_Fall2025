{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMtpXRbrNJ+IbvjA+722INe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/d-noe/NLP_DH_PSL_Fall2025/blob/main/code/1_bert_training/Discover_BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BERT: Bidirectional Encoder Representations from Transformers\n",
        "\n",
        "![](https://sesameworkshop.org/wp-content/uploads/2023/03/presskit_ss_bio_bert.png)\n",
        "\n",
        "Link to the original paper by [Devlin et al., 2019](https://aclanthology.org/N19-1423/).\n",
        "\n"
      ],
      "metadata": {
        "id": "F4UnNObpT_Ga"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set-up\n",
        "\n",
        "Install and import necessary Python libraries and modules.\n",
        "\n",
        "This notebook will mainly rely on [`transformers` Python library](https://huggingface.co/docs/transformers/installation) and, later on, we'll use [`bertviz`](https://github.com/jessevig/bertviz) to inspect attention mechanisms."
      ],
      "metadata": {
        "id": "j3Dtyxv0jGF0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ov8UT8eiKyEk"
      },
      "outputs": [],
      "source": [
        "! pip install bertviz transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Finds out if 'cuda' (i.e. GPU) is available\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "zX7j2npvKSK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Overview"
      ],
      "metadata": {
        "id": "CWxI2QqxczN3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import BertForMaskedLM, BertTokenizerFast\n",
        "\n",
        "# tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
        "# model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")"
      ],
      "metadata": {
        "id": "M-Q8Cbd8c1n8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For BERT\n",
        "from transformers import BertTokenizerFast, BertModel\n",
        "\n",
        "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
        "model = BertModel.from_pretrained(\"bert-base-uncased\").to(DEVICE) #"
      ],
      "metadata": {
        "id": "YERed9kzKAs4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Can you retrieve all the different components that we described earlier?"
      ],
      "metadata": {
        "id": "dO3k3-0zKMQX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "id": "c2nDeZmeII0Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.tokenize(\"BERT means Bidirectional Encoder Representations from Transformers.\")"
      ],
      "metadata": {
        "id": "h8HQWpCsJlrC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What can you observe? What is the *'atomic unit'*, or *linguistic event*, for BERT? Can you think why?"
      ],
      "metadata": {
        "id": "XRLFgiHeKBDI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Size of the vocabulary: {tokenizer.vocab_size}.\")"
      ],
      "metadata": {
        "id": "SrIxHzX_Lt8R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_input = tokenizer(\"BERT means Bidirectional Encoder Representations from Transformers.\")\n",
        "print(tokenizer.decode(tokenized_input[\"input_ids\"]))"
      ],
      "metadata": {
        "id": "XLdrRaI3Oo2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Is there anything particular in the tokenized / detokenized text?"
      ],
      "metadata": {
        "id": "cmBHD-XvO57x"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OljXUblLO7At"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Masked Language Modeling\n",
        "\n",
        "BERT uses a **“masked language model” (MLM)** pre-training objective, inspired by the Cloze task.\n",
        "\n",
        "During training, the masked language model randomly masks some of the tokens from the input, and the objective is to **predict** the original word of the masked word **based only on its context**. (Remember: *You shall know a word by the company it keeps* (Firth, J. R. 1957:11)).\n",
        "\n",
        "Unlike left-to-right language model pre-training (or causal language modeling objective), the MLM objective enables the representation to **fuse the left and the right context**, which allows us to pre-train a deep bidirectional Transformer.\n",
        "\n",
        "Now let's see how it does in practice! What will BERT predict?"
      ],
      "metadata": {
        "id": "Hlnb33anJf_l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model using `BertForMaskedLM`\n",
        "# --> appends a prediction head to the architecture\n",
        "# --> allows to perform MLM tasks (i.e. predict missing word)\n",
        "\n",
        "from transformers import BertForMaskedLM, BertTokenizerFast\n",
        "\n",
        "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
        "model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\").to(DEVICE)"
      ],
      "metadata": {
        "id": "fdlXVlwcJM2I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Can you spot the difference?\n",
        "print(model)"
      ],
      "metadata": {
        "id": "VGnRFdk4L4UH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Quick helper function\n",
        "# --> retrieve the IDs of the `n` most probable tokens based on model's logits\n",
        "\n",
        "def get_n_most_likely(\n",
        "    logits,\n",
        "    mask_token_id:int,\n",
        "    n:int=10,\n",
        "):\n",
        "  \"\"\"\n",
        "  Input:\n",
        "    logits: tensor of shape (batch_size, seq_len, vocab_size)\n",
        "    mask_token_id: id of the token to predict\n",
        "    n: number of most likely tokens to return\n",
        "  Output:\n",
        "    list of n most likely tokens ids\n",
        "  \"\"\"\n",
        "  return logits[0, mask_token_id].argsort()[-n:].cpu().numpy()[::-1]\n",
        "\n",
        "def get_masked_position(\n",
        "    token_ids,\n",
        "):\n",
        "  \"\"\"\n",
        "  Input\n",
        "    token_ids: list of token ids\n",
        "  Output\n",
        "    position of the [MASK] token\n",
        "  \"\"\"\n",
        "  # ([list with 'True' at MASK position, 'False' elsewhere]) --> convert numpy --> get 'True' (1) postion\n",
        "  return (token_ids == tokenizer.mask_token_id).cpu().numpy().argmax()"
      ],
      "metadata": {
        "id": "miIeRqaTD9H7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Enter your sentence here\n",
        "# - enter full `sentence` and `word_to_mask`\n",
        "# - or, directly write a sentence with a [MASK]\n",
        "\n",
        "sentence = \"The cat chased the mouse.\"\n",
        "word_to_mask = \"cat\"\n",
        "masked_sentence = sentence.replace(word_to_mask, \"[MASK]\")\n",
        "\n",
        "print(masked_sentence)\n"
      ],
      "metadata": {
        "id": "IxSkbfaMMBWB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's pass our masked sentence into the model.\n",
        "\n",
        "Remember, we first have to tokenize it to prepare the input. Then, we'll give our tokenized sentence to the model."
      ],
      "metadata": {
        "id": "WPLbwRWEMr50"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenize + put everything on the same DEVICE\n",
        "tokenized_inputs = tokenizer(masked_sentence, return_tensors=\"pt\").to(DEVICE)\n",
        "print(tokenized_inputs)"
      ],
      "metadata": {
        "id": "SSJnXM1oMqey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model(**tokenized_inputs)\n",
        "print(outputs)"
      ],
      "metadata": {
        "id": "WQBtMOJJNLnV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's retrieve the most probable tokens at the `[MASK]` position based on the model's output:"
      ],
      "metadata": {
        "id": "1YCvfeiZNT76"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We'll use the helper functions for this part\n",
        "\n",
        "mask_position = get_masked_position(tokenized_inputs[\"input_ids\"][0])\n",
        "\n",
        "predicted_token_id = get_n_most_likely(\n",
        "    logits = outputs.logits,\n",
        "    mask_token_id=mask_position,\n",
        "    n=1,\n",
        ")\n",
        "\n",
        "predicted_token = tokenizer.decode(predicted_token_id)\n",
        "\n",
        "print(f\"For the [MASK] in the sentence '{masked_sentence}',\")\n",
        "print(f\"the model predicts the token: '{predicted_token}'.\")"
      ],
      "metadata": {
        "id": "gALhIlM-NTkg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DCsk-lObP6lI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now you can try with different sentence in a single cell below:\n",
        "masked_sentence = \"Paris is the most [MASK] city in the world.\"\n",
        "n_most_likely = 10\n",
        "\n",
        "# We'll use the helper functions for this part\n",
        "\n",
        "tokenized_inputs = tokenizer(masked_sentence, return_tensors=\"pt\").to(DEVICE)\n",
        "outputs = model(**tokenized_inputs)\n",
        "mask_position = get_masked_position(tokenized_inputs[\"input_ids\"][0])\n",
        "\n",
        "predicted_token_id = get_n_most_likely(\n",
        "    logits = outputs.logits,\n",
        "    mask_token_id=mask_position,\n",
        "    n=n_most_likely,\n",
        ")\n",
        "\n",
        "print(f\"For the [MASK] in the sentence '{masked_sentence}',\")\n",
        "if n_most_likely == 1:\n",
        "  predicted_tokens = tokenizer.decode(predicted_token_id)\n",
        "  print(f\"the model predicts the token: '{predicted_tokens}'.\")\n",
        "else:\n",
        "  print(f\"the model predicts the tokens:\")\n",
        "  for i, t_id in enumerate(predicted_token_id):\n",
        "    print(f\"{i+1} - {tokenizer.decode(t_id)}\")\n"
      ],
      "metadata": {
        "id": "NsgmNlm-P686"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H0G03WGvROJZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 👁️ Exploring Attention with BERTviz\n",
        "\n",
        "When you run the cell below, an **interactive attention visualization** will appear.  \n",
        "This tool helps us **see how the model distributes its attention between tokens** in the sentence.\n",
        "\n",
        "---\n",
        "\n",
        "#### 🧩 Understanding What You See\n",
        "\n",
        "When you open the BERTviz `head_view`, you’ll notice several key components:\n",
        "\n",
        "1. **Layer Selection (Top Bar)**  \n",
        "   - DistilBERT has **6 layers** (numbered 0–5).  \n",
        "   - You can switch between layers to see how attention evolves.  \n",
        "   - Lower layers tend to capture **local relations** (like syntax), while higher layers focus on **global meaning**.\n",
        "\n",
        "2. **Head Selection (Colored Boxes)**  \n",
        "   - Each layer has **12 attention heads** (labeled 0–11).  \n",
        "   - Each head learns different patterns:\n",
        "     - Some connect nearby words.\n",
        "     - Some capture longer dependencies (e.g., subject → verb).\n",
        "     - Others gather sentence-wide context around special tokens like `[CLS]` or `[SEP]`.  \n",
        "   - Click the head boxes to toggle each head on or off.\n",
        "\n",
        "3. **Token Lists (Left and Right Columns)**  \n",
        "   - These columns show all the tokens (subword pieces) in the input sentence.  \n",
        "   - DistilBERT uses **WordPiece tokenization**, so some words appear split into parts (e.g., `\"arith\"`, `\"##metical\"`).  \n",
        "   - The same sentence is displayed on both sides — attention connects each token **to every other token**.\n",
        "\n",
        "4. **Attention Lines (Colored Arcs)**  \n",
        "   - Each line connects a **query token** (on the left) to a **key token** (on the right).  \n",
        "   - The color and thickness show **how strongly** the model attends from one token to another.  \n",
        "   - When multiple heads are active, lines from each head are color-coded according to the boxes at the top.\n",
        "\n",
        "---\n",
        "\n",
        "#### 🖱️ Hovering Tokens and Asymmetry\n",
        "\n",
        "- **Hover over a word on the left**: the lines show where this **query token** attends to **key tokens** on the right.  \n",
        "- **Hover over a word on the right**: the lines show which **query tokens** are attending to this **key token**.  \n",
        "\n",
        "⚠️ **Important:** Attention is **not necessarily symmetrical**.  \n",
        "- A token A may strongly attend to token B, but token B might attend more to some other token C.  \n",
        "- This is because attention is **directional**: each token has its own query vector and computes attention to keys independently.\n",
        "\n",
        "So, hovering left vs. right gives complementary, but **different perspectives** on the model’s attention patterns.\n",
        "\n",
        "---\n",
        "\n",
        "#### 🎓 How to Explore\n",
        "\n",
        "- **Hover over a word** on the left or right to see attention patterns.  \n",
        "- **Switch between layers** to see how the focus changes from local to global.  \n",
        "- **Toggle heads** to isolate different types of relationships.  \n",
        "\n",
        "Try this with our example sentence:\n",
        "\n",
        "> *“modern electronic calculators contain a keyboard with buttons for digit and arithmetical operations.”*\n",
        "\n",
        "Questions to guide your exploration:\n",
        "- Does *digit* attend to words like *arithmetical*, *keyboard*, or *electronic* in early layers?  \n",
        "- Do higher layers show attention concentrating on `[SEP]` or spreading evenly across tokens?  \n",
        "- Which layer seems to highlight meaningful semantic connections?\n",
        "\n",
        "---\n",
        "\n",
        "#### 🧠 A Note on Interpretation\n",
        "\n",
        "While BERTviz gives insight into model behavior, keep in mind:\n",
        "\n",
        "- **Attention ≠ Explanation**  \n",
        "  Attention shows *how* tokens interact, not necessarily *why* the model predicts something.  \n",
        "\n",
        "- **Later layers often focus on `[SEP]` or `[CLS]`**  \n",
        "  This is normal — these tokens serve as *summary anchors* for global information.\n",
        "\n",
        "- **Not every head is interpretable**  \n",
        "  Some heads track syntax or position rather than semantics.\n",
        "\n",
        "Use this visualization as a **qualitative exploration tool** to understand patterns, not as a definitive explanation of the model’s reasoning.\n"
      ],
      "metadata": {
        "id": "Aopef-ZPQI7n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For visualisation of attention mechanisms\n",
        "from bertviz import head_view, model_view"
      ],
      "metadata": {
        "id": "E5bm0HexKDyh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BertModel.from_pretrained(\n",
        "    \"bert-base-uncased\",\n",
        "    output_attentions=True,\n",
        "    device_map=DEVICE,\n",
        ")\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "id": "qSHHNqp_N9xL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"The animal didn't cross the street because it was too tired.\"\n",
        "inputs = tokenizer(sentence, return_tensors=\"pt\").to(DEVICE)\n",
        "outputs = model(**inputs)\n",
        "\n",
        "attentions = outputs.attentions  # Tuple of attention matrices, one per layer\n",
        "print(f\"Number of layers: {len(attentions)}\")\n",
        "print(f\"Shape of each attention tensor: {attentions[0].shape}\")  # (batch, num_heads, seq_len, seq_len)"
      ],
      "metadata": {
        "id": "UbrD4mzVcntx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title (Simulated) Masked Language Modeling\n",
        "\n",
        "# Hint: look at layer 10!\n",
        "sentence = \"The capital of France is [MASK].\"\n",
        "\n",
        "inputs = tokenizer.encode_plus(sentence, return_tensors='pt').to(DEVICE)\n",
        "outputs = model(**inputs)\n",
        "\n",
        "# Convert token ids to tokens\n",
        "tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
        "\n",
        "# Use head_view for single-sentence visualization\n",
        "head_view(attention=outputs.attentions, tokens=tokens)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "t6O9eHTLQedX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## *The animal couldn't cross the street because **it** ...*"
      ],
      "metadata": {
        "id": "DMHq00tsRAof"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ... *was too tired.*\n",
        "sentence = \"The animal couldn't cross the street because it was too tired.\"\n",
        "\n",
        "inputs = tokenizer.encode_plus(sentence, return_tensors='pt').to(DEVICE)\n",
        "outputs = model(**inputs)\n",
        "\n",
        "# Convert token ids to tokens\n",
        "tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
        "\n",
        "# Use head_view for single-sentence visualization\n",
        "head_view(attention=outputs.attentions, tokens=tokens)"
      ],
      "metadata": {
        "id": "xqw2o5eU9MmK",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ... *was too wide.*\n",
        "\n",
        "## hint: have a look at layer 9\n",
        "sentence = \"The animal couldn't cross the street because it was too wide.\"\n",
        "\n",
        "inputs = tokenizer.encode_plus(sentence, return_tensors='pt').to(DEVICE)\n",
        "outputs = model(**inputs)\n",
        "\n",
        "# Convert token ids to tokens\n",
        "tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
        "\n",
        "# Use head_view for single-sentence visualization\n",
        "head_view(attention=outputs.attentions, tokens=tokens)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "FBp3T9OcOpUI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_view(attention=outputs.attentions, tokens=tokens)"
      ],
      "metadata": {
        "id": "dL2Tlss8ObEL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1zzUIwjcPxn2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3U28iie6PxZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BERT for Feature Exctraction: Vector Representations\n",
        "\n",
        "You can use the pre-trained BERT to create contextualized word embeddings. Then you can feed these embeddings to your existing model – a process in the paper shows yield results not far behind fine-tuning BERT on a task such as named-entity recognition.\n",
        "\n",
        "![Extracted from https://jalammar.github.io/illustrated-bert/](https://jalammar.github.io/images/bert-contexualized-embeddings.png)\n",
        "\n",
        "There are different ways to extract embeddings from pre-trained model (remember `bertviz`, you can use different (combination of) layers, heads, etc.).\n",
        "\n",
        "Here we'll stick with a common approach, using the `last_hidden_state`: these vectors are the result of all layers' transformations and attention operations combined: they are what the model ultimately \"sees\" for each token."
      ],
      "metadata": {
        "id": "w2dkCbG-UBl2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model — no need for MLM here: we'll extract the embeddings\n",
        "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
        "model = BertModel.from_pretrained(\"bert-base-uncased\").to(DEVICE)"
      ],
      "metadata": {
        "id": "tW9-f22IMmCO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Toy example data: how does BERT understand the word 'field'?\n",
        "\n",
        "data = {\n",
        "    \"Zinedine Zidane is the greatest on the field.\": \"Football Pitch\",\n",
        "    \"On the field, Michel Platini was undeniably different.\": \"Football Pitch\",\n",
        "    \"Jonah Lomu reigned on the field.\": \"Rugby Pitch\",\n",
        "    \"Barry Bonds had no contender on the field.\": \"Baseball\",\n",
        "    \"The crop is growing in the field.\": \"Agriculture Field\",\n",
        "    \"This field was harvested by the farmer.\": \"Agriculture Field\",\n",
        "    \"He hopes to find work in the informatics field.\": \"Domain\",\n",
        "    \"Mary Frances Lyon is a pioneer in the field of genetic research.\": \"Domain\",\n",
        "}\n",
        "\n",
        "sentences = list(data.keys())\n",
        "senses = list(data.values())"
      ],
      "metadata": {
        "id": "L82Tflq87kxo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenize our sentences\n",
        "tokenized_sentences = tokenizer(\n",
        "  sentences,\n",
        "  truncation=True,\n",
        "  padding=True,\n",
        "  return_tensors=\"pt\"\n",
        ").to(DEVICE)"
      ],
      "metadata": {
        "id": "9vhs796Q8fTV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, sentence in enumerate(sentences):\n",
        "  print(f\"Sentence: {sentence}\")\n",
        "  for tokenized_id in tokenized_sentences['input_ids'][i]:\n",
        "    print(f\"\\t{tokenized_id} : {tokenizer.decode(tokenized_id)}\")"
      ],
      "metadata": {
        "id": "cUWHZaXg8p9g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What can you observe? What is the `token_id` of the word `'field'`?"
      ],
      "metadata": {
        "id": "IJj0GfnrVNZa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.vocab[\"field\"]"
      ],
      "metadata": {
        "id": "9KL5OPg5MsYZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model(**tokenized_sentences)"
      ],
      "metadata": {
        "id": "D5w7hq5L9g7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vector Representation\n",
        "embeddings = outputs[\"last_hidden_state\"]\n",
        "embeddings.shape # N samples, Sequence length, Embedding dimensions"
      ],
      "metadata": {
        "id": "uXxgTT5Y9kWm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Quick and dirty way to extract the embeddings at the position of the word 'field'\n",
        "import numpy as np\n",
        "\n",
        "word_id = 2492\n",
        "\n",
        "field_token_pids = [np.argmax(t.cpu().numpy()==word_id) for t in tokenized_sentences[\"input_ids\"]]\n",
        "\n",
        "field_vectors = np.array([\n",
        "    o[p_id].detach().cpu().numpy()\n",
        "    for o, p_id in zip(outputs[\"last_hidden_state\"], field_token_pids)\n",
        "])\n",
        "\n",
        "field_vectors.shape # N samples, Embedding dimensions"
      ],
      "metadata": {
        "id": "IkTkPPD292z6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Embeddings PCA Visualisation:\n",
        "\n",
        "from sklearn.manifold import MDS\n",
        "from sklearn.metrics.pairwise import cosine_distances\n",
        "from sklearn.decomposition import PCA\n",
        "import altair as alt\n",
        "import pandas as pd\n",
        "\n",
        "#dim_reducer = MDS(n_components=2, dissimilarity=\"precomputed\")\n",
        "dim_reducer = PCA(n_components=2)\n",
        "\n",
        "df_field = pd.DataFrame()\n",
        "df_field[\"sentence\"] = sentences\n",
        "df_field[\"sense\"] = senses\n",
        "\n",
        "#reduced_vectors = dim_reducer.fit_transform(cosine_distances(field_vectors))\n",
        "reduced_vectors = dim_reducer.fit_transform(field_vectors)\n",
        "\n",
        "df_field[\"Dim_1\"] = reduced_vectors[:,0]\n",
        "df_field[\"Dim_2\"] = reduced_vectors[:,1]\n",
        "\n",
        "chart = alt.Chart(\n",
        "    df_field,\n",
        "    title=f\"Field Embeddings\"\n",
        ").mark_circle(size=200).encode(\n",
        "    alt.X('Dim_1',\n",
        "        scale=alt.Scale(zero=False)\n",
        "    ),\n",
        "    y=\"Dim_2\",\n",
        "    color= \"sense\",\n",
        "    tooltip=['sentence'],\n",
        "    ).interactive().properties(\n",
        "    width=500,\n",
        "    height=500\n",
        ")\n",
        "\n",
        "chart"
      ],
      "metadata": {
        "id": "v7DsQgDI9xlT",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bingo! The model produces vector representations that are closer in the embedding space when the word is used with a similar meaning!\n",
        "\n",
        "\n",
        "Remember `word2vec`, would you have been able to produce such results?\n"
      ],
      "metadata": {
        "id": "yoUsH5leWKxP"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7oMeZLjKNTPG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}