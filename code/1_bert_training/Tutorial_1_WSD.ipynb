{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMCHiAK+TzUEBUf5el7eKy5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/d-noe/NLP_DH_PSL_Fall2025/blob/main/code/1_bert_training/Tutorial_1_WSD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tutorial Session 1: Word Sense Disambiguation (WSD) with BERT\n",
        "\n",
        "![](https://external-preview.redd.it/3On3o8P2JG1enhtJKsbEidkaP_YeHxuRhXR32QHFhkA.png?width=1080&crop=smart&auto=webp&s=78ee696f5971df43fa1cfc925de9293b60d879e3)\n",
        "\n",
        "\n",
        "The goal of this notebook is to use the representational power of pre-trained (encoder-only) models to explore Word Sense Disambiguation (WSD).\n",
        "\n",
        "Note that this tutorial doesn't aim to go through the whole pipeline of WSD (no classification or 'sense-retrieval'), but more to illustrate part of it through the use of BERT-like models.\n"
      ],
      "metadata": {
        "id": "F4UnNObpT_Ga"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title What we are aiming for:\n",
        "\n",
        "\n",
        "from IPython.core.display import display, HTML\n",
        "import urllib.request\n",
        "\n",
        "fp = urllib.request.urlopen(\"https://raw.githubusercontent.com/d-noe/NLP_DH_PSL_Fall2025/refs/heads/main/props/digit.html\")\n",
        "mybytes = fp.read()\n",
        "\n",
        "mystr = mybytes.decode(\"utf8\")\n",
        "fp.close()\n",
        "\n",
        "display(HTML(mystr))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "eydJGI3Vgggj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set-up\n",
        "\n",
        "Install and import necessary Python libraries and modules.\n",
        "\n",
        "This notebook will mainly rely on [`transformers` Python library](https://huggingface.co/docs/transformers/installation), we will also use [`scikit-learn`](https://scikit-learn.org/stable/install.html) for dimensionality reduction, and [`altair`](https://altair-viz.github.io/) for visualisation."
      ],
      "metadata": {
        "id": "j3Dtyxv0jGF0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ov8UT8eiKyEk"
      },
      "outputs": [],
      "source": [
        "! pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For BERT --> using DistilBert, a smaller model that retains most of its power\n",
        "from transformers import DistilBertTokenizerFast, DistilBertModel\n",
        "import torch\n",
        "\n",
        "# For data manipulation and analysis\n",
        "import pandas as pd\n",
        "pd.options.display.max_colwidth = 200\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# For interactive data visualization\n",
        "import altair as alt"
      ],
      "metadata": {
        "id": "rTO3dOrfVqGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# are 'GPU's available?\n",
        "# If using Colab, you can change your runtime to access GPUs\n",
        "# but it is not really needed here, computations are not too long on CPUs\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "zX7j2npvKSK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Model"
      ],
      "metadata": {
        "id": "vl8SFkJEctAF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = DistilBertModel.from_pretrained(\n",
        "    \"distilbert-base-uncased\",\n",
        "    output_attentions=True,\n",
        "    device_map=DEVICE,\n",
        ")\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')"
      ],
      "metadata": {
        "id": "ZL87yu7wcw-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "id": "ynoKtuQcc6Rz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Loading\n",
        "\n",
        "The dataset used for this tutorial is based on the [CoarseWSD-20 dataset](https://github.com/danlou/bert-disambiguation/tree/master) introduced in *Analysis and Evaluation of Language Models for Word Sense Disambiguation* [(Loureiro et al., 2021)](https://arxiv.org/abs/2008.11608).\n",
        "\n",
        "The original dataset is based on Wikipedia articles excerpts, developed to evaluate and train word disambiguation models based on a selection of 20 expert-selected words (apple, club, bow, bank, crane, square, chair, java, bass, seal, pitcher, arm, mole, pound, deck, trunk, spring, hood, yard, digit).\n",
        "\n",
        "In the context of this tutorial, it is presented in the form of a `.csv` file, which can be conviniently loaded as a `pandas.DataFrame`. This file contains the following columns:\n",
        "- `text` [string]: uncased sentence containing an instance of a polysemeous word (example: *'some displays can show only digit or alphanumeric characters .'*)\n",
        "- `target` [string]: the polysemeous word to be considered in `text` (example: *'digit'*)\n",
        "- `label` [int]: the label of the sense associated with the target (example: *0*)\n",
        "- `label_sense` [str]: a human-understandable label to disambiguate the target (example: *'digit_numerical_digit'*)\n",
        "- `split` [str]: original data split, i.e. train or test (example: *'test'*)."
      ],
      "metadata": {
        "id": "RnzYX-0rWjY4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/d-noe/NLP_DH_PSL_Fall2025/refs/heads/main/code/scripts/helpers.py\n",
        "from helpers import load_csv_from_github\n",
        "\n",
        "# Load the data\n",
        "wsd_df = load_csv_from_github(\"data/word_sense/CoarseWSD-20.csv\")\n",
        "print(f\"The DataFrame contains {len(wsd_df)} rows.\")\n",
        "\n",
        "# Display 5 random rows\n",
        "wsd_df.sample(5)"
      ],
      "metadata": {
        "id": "GyLO4eSNAkpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# See number of unique examples per target and number of senses\n",
        "wsd_df[[\"target\", \"label\"]].groupby([\"target\"]).agg(['count', 'nunique'])"
      ],
      "metadata": {
        "id": "ttWX9SR3byoM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the purpose of this experiment, let's focus on a sample of the original dataset. As we are interested in exploring the role of context in the vector representations produced by the model, we will select only one `focus_word`, and randomly sample `n_samples` (or less if less is available) sentences."
      ],
      "metadata": {
        "id": "Q7-A_kqoftW7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "focus_word = \"mole\"\n",
        "n_samples = 1000\n",
        "\n",
        "# Select rows associated with the focus word\n",
        "df_focus = wsd_df[wsd_df[\"target\"]==focus_word]\n",
        "# Randomly sample from the DataFrame (if enough data)\n",
        "if len(df_focus) > n_samples:\n",
        "  df_focus = df_focus.sample(n_samples)\n",
        "else:\n",
        "  print(f\"{n_samples} larger than the total amount of data available for {focus_word}: {len(df_focus)}.\")\n",
        "\n",
        "# Print some information about our dataset\n",
        "unique_senses = df_focus[\"label_sense\"].unique()\n",
        "print(f\"Total number of senses associated with '{focus_word}': {len(unique_senses)}\")\n",
        "for sense in unique_senses:\n",
        "  print(f\"\\t{sense}\")\n",
        "  print(f\"\\t\\tProportion of the data: {100*len(df_focus[df_focus['label_sense']==sense])/len(df_focus):.0f}%.\")\n",
        "  print(f\"\\t\\tExample: {df_focus[df_focus['label_sense']==sense].iloc[0]['text']}\")"
      ],
      "metadata": {
        "id": "XKhK6KFogNQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Contextualized word-embeddings\n",
        "\n",
        "We will `batch` the examples, this allows to process several examples at once, and to speed up the process.\n",
        "\n",
        "Then in a loop, for `batches` we will:\n",
        "- tokenize the texts\n",
        "- run the tokenized texts through the model\n",
        "- store:\n",
        "  - the tokenized texts IDs (this will allow us to retrieve the position of the word of interest)\n",
        "  - the embeddings produced by the model"
      ],
      "metadata": {
        "id": "_G1qiAgrVwG9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texts = list(df_focus['text'])\n"
      ],
      "metadata": {
        "id": "7cFjCRVEcGz5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "batch_size = 16 # adjust for memory usage\n",
        "max_length = 256  # adjust as needed\n",
        "\n",
        "tokenized_texts_ids = []\n",
        "all_last_hidden_states = []\n",
        "\n",
        "for i in tqdm(range(0, len(texts), batch_size)):\n",
        "    batch_texts = texts[i:i+batch_size]\n",
        "    tokenized = tokenizer(\n",
        "        batch_texts,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",  # pads every batch to the same length\n",
        "        max_length=max_length,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**tokenized)\n",
        "\n",
        "    all_last_hidden_states.append(outputs.last_hidden_state.cpu())\n",
        "    tokenized_texts_ids.append(tokenized[\"input_ids\"].cpu())\n",
        "\n",
        "# Now safe to concatenate\n",
        "last_hidden_state = torch.cat(all_last_hidden_states, dim=0)\n",
        "tokenized_texts_ids = torch.cat(tokenized_texts_ids, dim=0)"
      ],
      "metadata": {
        "id": "OA81Y108M5Le"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "last_hidden_state.shape # N_samples x Sequence Length x Embedding Dimension"
      ],
      "metadata": {
        "id": "tqBdM83hNhgK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Retrieving words vector representations\n",
        "\n",
        "Then, we will retrieve the vectors associated with the `focus_word` we chose in each of the samples."
      ],
      "metadata": {
        "id": "NaS5nWc8WU61"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# quick and ugly heuristic to retrieve vectors associated with <focus_word> tokens\n",
        "\n",
        "# 0. Isolate token id from model vocabulary\n",
        "word_id = tokenizer(focus_word)[\"input_ids\"][1]\n",
        "\n",
        "# 1. Retrieve focus token positions in tokenized texts\n",
        "word_token_pids = [np.argmax(t.numpy()==word_id) for t in tokenized_texts_ids]\n",
        "\n",
        "# 2. Extract vectors at these positions\n",
        "word_vectors = np.array([\n",
        "    o[p_id].numpy()\n",
        "    for o, p_id in zip(last_hidden_state, word_token_pids)\n",
        "])"
      ],
      "metadata": {
        "id": "lnBlBbh_NHEd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_vectors.shape # N_samples x Embedding Dimensions"
      ],
      "metadata": {
        "id": "aLLwrR8-CkSk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now have the **embeddings** for the words in each of the examples!\n",
        "\n",
        "Let's visualise it (in 2D, after dimensionality reduction, not 768D), to explore the semantic space produced by the model!"
      ],
      "metadata": {
        "id": "TJ-UDbF5dm-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PCA Visualisation"
      ],
      "metadata": {
        "id": "HjthhZk3Wd1p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA(n_components=2)\n",
        "\n",
        "# Just for convenience (+altair integration): store the data in a DataFrame\n",
        "df_plot = df_focus.copy()\n",
        "\n",
        "reduced_vectors = pca.fit_transform(word_vectors)\n",
        "\n",
        "df_plot[\"PCA_1\"] = reduced_vectors[:,0]\n",
        "df_plot[\"PCA_2\"] = reduced_vectors[:,1]\n"
      ],
      "metadata": {
        "id": "Jf2milx1WgBy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chart = alt.Chart(\n",
        "    df_plot,\n",
        "    title=f\"Word PCA Distribution: {focus_word}\"\n",
        ").mark_circle(size=200).encode(\n",
        "    alt.X('PCA_1',\n",
        "        scale=alt.Scale(zero=False)\n",
        "    ),\n",
        "    y=\"PCA_2\",\n",
        "    color= \"label_sense\",\n",
        "    tooltip=['target', 'label_sense', 'text'],\n",
        "    ).interactive().properties(\n",
        "    width=500,\n",
        "    height=500\n",
        ")\n",
        "chart"
      ],
      "metadata": {
        "id": "OLSeyaefjhoD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "85p1hPvgCwHL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "evk8vvv6Cv98"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}