{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNFI0Ypqoc2a1sTbsdmYe+v",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/d-noe/NLP_DH_PSL_Fall2025/blob/main/code/2_topic_modeling/Tutorial_2_MyBERTopic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Topic Modeling\n",
        "\n",
        "Implement your own BERTopic!\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img alt=\"Directions for Cookery, in its Various Branches by Eliza Leslie\" src=\"https://maartengr.github.io/BERTopic/algorithm/modularity.svg\" width=\"100%\">\n",
        "</p>\n",
        "<p align=\"right\">\n",
        "  <a href=https://maartengr.github.io/BERTopic/algorithm/algorithm.html>Source: BERTopic â€” Algorithm</a>\n",
        "</p>\n"
      ],
      "metadata": {
        "id": "jt-0qpAn0X1a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install transformers"
      ],
      "metadata": {
        "id": "1gt_N6xm0fVG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import umap\n",
        "import hdbscan\n",
        "import altair as alt\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "from sentence_transformers import SentenceTransformer"
      ],
      "metadata": {
        "id": "9qpdN8jQ_XtE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/d-noe/NLP_DH_PSL_Fall2025/refs/heads/main/code/scripts/helpers.py\n",
        "from helpers import load_csv_from_github\n"
      ],
      "metadata": {
        "id": "Ptpr0m2WlouH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data\n",
        "recipes_df = load_csv_from_github(\"data/topic_data/nineteenth_recipes.csv\")\n",
        "\n",
        "# Display 5 random rows\n",
        "recipes_df.sample(5)"
      ],
      "metadata": {
        "id": "0dxAy-15mFoH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample 100 documents to test during development\n",
        "docs_dev = recipes_df[\"text\"].sample(100).tolist()\n",
        "\n",
        "print(len(docs_dev))"
      ],
      "metadata": {
        "id": "VoYstYN7_DOI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# âš™ï¸ `MyBERTopic`\n",
        "\n",
        "\n",
        "Let's build our own (simplified) version of [BERTopic](https://maartengr.github.io/BERTopic/algorithm/algorithm.html). Remember BERTopic is a *Transformer-Based Topic Model*. It can be used to discover major themes or subjects discussed in any text corpora.\n",
        "\n",
        "The algorithm implemented by BERTopic relies on 5 to 6 steps that can be decomposed as:\n",
        "\n",
        "A. **Creating topic clusters**\n",
        "\n",
        "1. Embedding\n",
        "2. Dimensionality reduction\n",
        "3. Clustering\n",
        "\n",
        "B. **Refining topic representation**\n",
        "\n",
        "4. Tokenizing\n",
        "5. Weighting scheme\n",
        "6. [Opt. Fine-tuning]\n",
        "\n",
        "We'll first start building the different components of the two major blocks, and then we'll encapsulate everything in a `class`."
      ],
      "metadata": {
        "id": "EsyDYFYoxOic"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A. Creating Clusters\n",
        "\n",
        "The cluster creation process requires three steps:\n",
        "1. Embedding the documents\n",
        "2. Performing dimensionality reduction\n",
        "3. Finding clusters based on the documents' representations\n",
        "\n",
        "We will stick to the default implementation of BERTopic, thus using the following components:\n",
        "\n",
        "|Task|Method|Implementation|\n",
        "|:---|:---|:---|\n",
        "|1. Embedding|SentenceBERT|[`sentence_transformers`](https://sbert.net/) ([`all-MiniLM-L6-v2` model](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2)|\n",
        "|2. Dimensionality reduction|UMAP|[`umap.UMAP`](https://umap-learn.readthedocs.io/en/latest/)|\n",
        "|3. Clustering|HDBSCAN|[`hdbscan.HDBSCAN`](https://hdbscan.readthedocs.io/en/latest/)|"
      ],
      "metadata": {
        "id": "5-b6PcVX4Smp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Embedding"
      ],
      "metadata": {
        "id": "czKfNZnl4W6R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load SBERT\n",
        "sentence_transformer_name = \"all-MiniLM-L6-v2\"\n",
        "\n",
        "embedder = SentenceTransformer(sentence_transformer_name)"
      ],
      "metadata": {
        "id": "9JWIuJcn4VqW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run SBERT on corpus\n",
        "embeddings = embedder.encode(docs_dev)\n",
        "\n",
        "embeddings.shape # N documents x Embedding Dimension"
      ],
      "metadata": {
        "id": "z5H6yXUX_xAU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Dimensionality Reduction"
      ],
      "metadata": {
        "id": "CK-4NGsf3iwo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate UMAP\n",
        "reducer = umap.UMAP(\n",
        "    n_neighbors=15, n_components=5, min_dist=0.0, metric='cosine', random_state=0\n",
        ")"
      ],
      "metadata": {
        "id": "avhGr9EA3j9V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reduce Embeddings dimensionality\n",
        "reduced_embeddings = reducer.fit_transform(embeddings)\n",
        "\n",
        "reduced_embeddings.shape # N documents x 5D\n"
      ],
      "metadata": {
        "id": "D6W739kFAInd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Clustering"
      ],
      "metadata": {
        "id": "KXDlq3bw3iq0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate HDBSCAN\n",
        "clusterer = hdbscan.HDBSCAN(\n",
        "    min_cluster_size=10, metric=\"euclidean\", cluster_selection_method=\"eom\"\n",
        ")"
      ],
      "metadata": {
        "id": "qvWwgijfAlHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform clustering on reduced 5D-embeddings\n",
        "clusters = clusterer.fit_predict(reduced_embeddings)\n",
        "\n",
        "clusters.shape # N documents x\n"
      ],
      "metadata": {
        "id": "Asm_pIMbA24F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(clusters[:10]) # cluster IDs associated with first 10 documents (Remember with HDBSCAN: -1 = noise)"
      ],
      "metadata": {
        "id": "9UaAjZU33jbs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have succesfully clustered our documents into different groups!\n",
        "\n",
        "The next step is to devise 'topic representations' for our cluster of documents so that we can understand what are the common themes."
      ],
      "metadata": {
        "id": "jOKFL6aVF9gZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## B. Refining topic representations\n",
        "\n",
        "Now that we have clustered the documents based on their similarities in the semantic space produced by SBERT, we need to find what characterizes these clusters, and to have representations that we, humans, can understand and interpret.\n",
        "\n",
        "To do so, we'll characterize topcis (clusters of documents) by their most prevalent words (with relation to other clusters).\n",
        "\n",
        "Once again, we stick to a (simplified) version of the original implementation (mainly, using raw TF-IDF instead of c-TF-IDF):\n",
        "\n",
        "|Task|Method|Implementation|\n",
        "|:---|:---|:---|\n",
        "|4. Vectorizing|uni- and bi- gram count|[`sklearn.feature_extraction.text.CountVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)|\n",
        "|5. Weighting Scheme|TFIDF|[`sklearn.feature_extraction.text.TfidfTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html)|"
      ],
      "metadata": {
        "id": "BHZVLhyH3ina"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Tokenizing"
      ],
      "metadata": {
        "id": "9Q16oZ3X3Vb7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate vectorizer: uni- and bi-gram count\n",
        "vectorizer = CountVectorizer(\n",
        "    stop_words=\"english\", ngram_range=(1, 2), min_df=2\n",
        ")"
      ],
      "metadata": {
        "id": "Yin2-Gz51Uhj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# [Technicalities] Join the texts for each topics into a dict: {topic: ALL DOCS GROUPED}\n",
        "dict_topics = {\n",
        "    topic:\" \".join(np.array(docs_dev)[np.array(clusters)==topic])\n",
        "    for topic in sorted(list(set(clusters)))\n",
        "}"
      ],
      "metadata": {
        "id": "Q6cD1yi2CHzI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorize the texts contained in each topic\n",
        "X = vectorizer.fit_transform(dict_topics.values())\n",
        "\n",
        "X.shape # N topics x N vocabulary"
      ],
      "metadata": {
        "id": "mv2LcfHyC6pQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Store vocabulary\n",
        "words = np.array(vectorizer.get_feature_names_out())"
      ],
      "metadata": {
        "id": "YAlC7mVOFFWF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Weighting Scheme"
      ],
      "metadata": {
        "id": "qwoc4-zs4LDP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate weight scheme: TFIDF\n",
        "weight_scheme = TfidfTransformer()"
      ],
      "metadata": {
        "id": "Y0K6K0ih1UY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform TFIDF\n",
        "tfidf = weight_scheme.fit_transform(X)\n",
        "\n",
        "tfidf.shape # N topics x N vocabulary (--> reordered)"
      ],
      "metadata": {
        "id": "-tM7btATD6a4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve top words for each topic and store in a dict\n",
        "topic_words_dict = {\n",
        "    i-1: words[np.argsort(tfidf_vector.toarray()[0])[::-1][:10]] # words[top indices]\n",
        "    for i, tfidf_vector in enumerate(tfidf)\n",
        "}\n",
        "topic_words_dict"
      ],
      "metadata": {
        "id": "iu-YRjAI3btt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "âœ¨ That's it, we can represent our topics using the most salient words in these documents!\n",
        "\n",
        "We have implemented all the building blocks of a `BERTopic` model, we can now encaspulate it in a class, and explore our full dataset."
      ],
      "metadata": {
        "id": "O5lU4JQmGYcH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸš€ Encapsulating everything into the `MyBERTopic` class\n",
        "\n",
        "We put everything in a class so that it is easy to use afterwards.\n",
        "\n",
        "And while we're at it, let's also add some helper functions to explore the data, and especially to visualise the documents embeddings and the dicsovered topics!"
      ],
      "metadata": {
        "id": "Z4qo-vrs3cK5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import umap\n",
        "import hdbscan\n",
        "import altair as alt\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "\n",
        "class MyBERTopic:\n",
        "    def __init__(\n",
        "        self,\n",
        "        sentence_transformer_name: str = \"all-MiniLM-L6-v2\",\n",
        "        umap_model=None,\n",
        "        hdbscan_model=None,\n",
        "        vectorizer_model=None,\n",
        "        weight_scheme=None,\n",
        "        verbose: bool = False,\n",
        "        seed : int = 92\n",
        "    ):\n",
        "        self.verbose = verbose\n",
        "        self.seed = seed\n",
        "\n",
        "        # --- Components ---\n",
        "        self.embedder = SentenceTransformer(sentence_transformer_name)\n",
        "\n",
        "        self.reducer = umap_model or umap.UMAP(\n",
        "            n_neighbors=15, n_components=5, min_dist=0.0, metric='cosine', random_state=self.seed\n",
        "        )\n",
        "\n",
        "        self.clusterer = hdbscan_model or hdbscan.HDBSCAN(\n",
        "            min_cluster_size=10, metric=\"euclidean\", cluster_selection_method=\"eom\"\n",
        "        )\n",
        "\n",
        "        self.vectorizer = vectorizer_model or CountVectorizer(\n",
        "            stop_words=\"english\", ngram_range=(1, 2), min_df=2\n",
        "        )\n",
        "\n",
        "        self.weight_scheme = weight_scheme or TfidfTransformer()\n",
        "\n",
        "        # Storage\n",
        "        self.documents_ = None\n",
        "        self.embeddings_ = None\n",
        "        self.reduced_embeddings_ = None\n",
        "        self.topics_ = None\n",
        "        self.topic_words_ = None\n",
        "        self.topic_words_weights_ = None\n",
        "        self.doc_topic_df_ = None\n",
        "\n",
        "    # =======================================================\n",
        "    #  Core Pipeline\n",
        "    # =======================================================\n",
        "\n",
        "    def _embed(self, documents: list[str]):\n",
        "        if self.verbose:\n",
        "            print(\"Embedding documents ...\")\n",
        "        return self.embedder.encode(documents, show_progress_bar=self.verbose)\n",
        "\n",
        "    def _reduce(self, embeddings: np.ndarray):\n",
        "        if self.verbose:\n",
        "            print(\"Reducing embeddings ...\")\n",
        "        return self.reducer.fit_transform(embeddings)\n",
        "\n",
        "    def _cluster(self, reduced_embeddings: np.ndarray):\n",
        "        if self.verbose:\n",
        "            print(\"Clustering reduced embeddings ...\")\n",
        "        return self.clusterer.fit_predict(reduced_embeddings)\n",
        "\n",
        "    def _tokenize(self, documents:list[str], topics: np.ndarray):\n",
        "        if self.verbose:\n",
        "            print(\"Tokenizing clustered texts...\")\n",
        "\n",
        "        dict_topics = { # join all document within topic (sorted)\n",
        "            topic:\" \".join(np.array(documents)[np.array(topics)==topic])\n",
        "            for topic in sorted(list(set(topics)))\n",
        "        }\n",
        "\n",
        "        X = self.vectorizer.fit_transform(dict_topics.values())\n",
        "        words = np.array(self.vectorizer.get_feature_names_out())\n",
        "        return X, words\n",
        "\n",
        "    def _weight(self,topics:np.ndarray, X:np.ndarray, words:np.ndarray, n_words:int=10):\n",
        "        if self.verbose:\n",
        "            print(\"Extracting topic words...\")\n",
        "        weights = self.weight_scheme.fit_transform(X)\n",
        "        ordered_topics = sorted(list(set(topics)))\n",
        "        topic_words_dict = {\n",
        "            ordered_topics[i]: words[np.argsort(weight_vector.toarray()[0])[::-1][:n_words]] # topic : words[top indices]\n",
        "            for i, weight_vector in enumerate(weights)\n",
        "        }\n",
        "        topic_words_weights_dict = {\n",
        "            ordered_topics[i]: np.sort(weight_vector.toarray()[0])[::-1][:n_words]\n",
        "            for i, weight_vector in enumerate(weights)\n",
        "        }\n",
        "        return topic_words_dict, topic_words_weights_dict\n",
        "\n",
        "    def _extract_topic_words(self, documents: list[str], topics: np.ndarray):\n",
        "        \"\"\"Compute topic keywords using a simplified c-TF-IDF approach.\"\"\"\n",
        "        X, words = self._tokenize(documents, topics)\n",
        "        topic_words_dict, topic_words_weights_dict = self._weight(topics=topics, X=X, words=words)\n",
        "\n",
        "        return topic_words_dict, topic_words_weights_dict\n",
        "\n",
        "    # =======================================================\n",
        "    #  Fit / Transform\n",
        "    # =======================================================\n",
        "\n",
        "    def fit(self, documents: list[str]):\n",
        "        self.documents_ = documents\n",
        "        self.embeddings_ = self._embed(documents)\n",
        "        self.reduced_embeddings_ = self._reduce(self.embeddings_)\n",
        "        self.topics_ = self._cluster(self.reduced_embeddings_)\n",
        "        self.topic_words_, self.topic_words_weights_ = self._extract_topic_words(documents, self.topics_)\n",
        "        self._build_doc_topic_df()\n",
        "        return self\n",
        "\n",
        "    def transform(self, new_documents: list[str]):\n",
        "        \"\"\"Assign topics to new docs based on nearest cluster (simple heuristic).\"\"\"\n",
        "        new_embeddings = self._embed(new_documents)\n",
        "        reduced = self.reducer.transform(new_embeddings)\n",
        "        labels, strengths = self.clusterer.approximate_predict(reduced)\n",
        "        return labels, strengths\n",
        "\n",
        "    def fit_transform(self, documents: list[str]):\n",
        "        self.fit(documents)\n",
        "        return self.topics_\n",
        "\n",
        "    # =======================================================\n",
        "    #  Helper Methods\n",
        "    # =======================================================\n",
        "\n",
        "    def _build_doc_topic_df(self):\n",
        "        self.doc_topic_df_ = pd.DataFrame({\n",
        "            \"document\": self.documents_,\n",
        "            \"topic\": self.topics_\n",
        "        })\n",
        "        return self.doc_topic_df_\n",
        "\n",
        "    # =======================================================\n",
        "    #  Visualization Utilities\n",
        "    # =======================================================\n",
        "\n",
        "    def visualise_embeddings(\n",
        "        self,\n",
        "        filter_out_noise=False,\n",
        "        metadata:dict=None,\n",
        "    ):\n",
        "        df = pd.DataFrame({\n",
        "            \"Dim_1\": self.reduced_embeddings_[:, 0],\n",
        "            \"Dim_2\": self.reduced_embeddings_[:, 1],\n",
        "            \"cluster\": self.topics_,\n",
        "            \"text\": self.documents_,\n",
        "        })\n",
        "\n",
        "        # if provided metadata: add it to df + tooltip\n",
        "        tooltip_vars = [\"text\", \"cluster\"]\n",
        "        if not metadata is None:\n",
        "            for k, v in metadata.items():\n",
        "                df[k] = v\n",
        "                tooltip_vars += [k]\n",
        "\n",
        "        if filter_out_noise:\n",
        "            df = df[df.cluster != -1]\n",
        "\n",
        "        chart = (\n",
        "            alt.Chart(df)\n",
        "            .mark_circle(size=100)\n",
        "            .encode(\n",
        "                x=\"Dim_1\",\n",
        "                y=\"Dim_2\",\n",
        "                color=\"cluster:N\",\n",
        "                tooltip=tooltip_vars[::-1],\n",
        "            )\n",
        "            .interactive()\n",
        "            .properties(width=600, height=500, title=\"Embedding Clusters\")\n",
        "        )\n",
        "        return chart\n",
        "\n",
        "    def visualize_barchart(self, top_n: int = 10):\n",
        "        if self.topic_words_ is None or self.topic_words_weights_ is None:\n",
        "            raise ValueError(\"Model must be fitted before visualization.\")\n",
        "\n",
        "        # 1. Build tidy dataframe\n",
        "        records = []\n",
        "        for topic, words in self.topic_words_.items():\n",
        "            weights = np.array(self.topic_words_weights_[topic]).flatten()\n",
        "            top_indices = np.argsort(weights)[::-1][:top_n]\n",
        "            for i in top_indices:\n",
        "                records.append({\n",
        "                    \"topic\": topic,\n",
        "                    \"word\": words[i],\n",
        "                    \"weight\": float(weights[i])\n",
        "                })\n",
        "\n",
        "        df = pd.DataFrame(records)\n",
        "\n",
        "        # Remove noise topic if exists (-1)\n",
        "        df = df[df[\"topic\"] != -1]\n",
        "\n",
        "        # 2. Define chart\n",
        "        base = (\n",
        "            alt.Chart(df)\n",
        "            .mark_bar()\n",
        "            .encode(\n",
        "                x=alt.X(\"weight:Q\", title=\"Word Weight\", scale=alt.Scale(domain=(0, df[\"weight\"].max() * 1.1))),\n",
        "                y=alt.Y(\"word:N\", sort=\"-x\", title=None),\n",
        "                tooltip=[\"topic:N\", \"word:N\", alt.Tooltip(\"weight:Q\", format=\".4f\")],\n",
        "                color=alt.Color(\"topic:N\", legend=None),\n",
        "            )\n",
        "            .properties(width=300, height=200)\n",
        "        )\n",
        "\n",
        "        # 3. Make facets per topic\n",
        "        chart = (\n",
        "            base\n",
        "            .facet(\n",
        "                facet=alt.Facet(\"topic:N\", title=\"Topic\"),\n",
        "                columns=3  # adjust layout\n",
        "            )\n",
        "            .resolve_scale(x=\"independent\", y=\"independent\")\n",
        "            .properties(title=f\"Top {top_n} Words per Topic\")\n",
        "        )\n",
        "\n",
        "        return chart\n",
        "\n",
        "\n",
        "    # =======================================================\n",
        "    #  Topic-Level Visualization\n",
        "    # =======================================================\n",
        "\n",
        "    def visualize_topics(\n",
        "        self,\n",
        "        top_n_words: int = 5,\n",
        "        from_reduced_embeddings: bool = False,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Visualize topics in 2D space based on their average embeddings.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        top_n_words : int\n",
        "            Number of top words to display for each topic.\n",
        "        \"\"\"\n",
        "        if self.topics_ is None or self.embeddings_ is None:\n",
        "            raise ValueError(\"Model must be fitted before visualization.\")\n",
        "\n",
        "        # Filter out noise (-1)\n",
        "        valid_idx = self.topics_ != -1\n",
        "        docs = np.array(self.documents_)[valid_idx]\n",
        "        topics = self.topics_[valid_idx]\n",
        "\n",
        "        # --- 1. Compute topic embeddings (average of doc embeddings)\n",
        "        if from_reduced_embeddings:\n",
        "            embeddings = self.reduced_embeddings_[valid_idx]\n",
        "            embeddings = MinMaxScaler().fit_transform(embeddings)\n",
        "        else:\n",
        "            embeddings = self.embeddings_[valid_idx]\n",
        "\n",
        "        topic_ids = np.unique(topics)\n",
        "        topic_embeddings = np.array([\n",
        "            embeddings[topics == t].mean(axis=0) for t in topic_ids\n",
        "        ])\n",
        "\n",
        "        # + Compute topics sizes\n",
        "        topic_sizes = np.array([\n",
        "            np.sum(np.array(topics)==t) for t in topic_ids\n",
        "        ])\n",
        "\n",
        "        # --- 2. Reduce to 2D for visualization IF not using reduced embeddings\n",
        "        if not from_reduced_embeddings:\n",
        "            reducer = umap.UMAP(\n",
        "                n_neighbors=2, n_components=2, metric=\"cosine\", min_dist=0.0, random_state=self.seed\n",
        "            )\n",
        "            topic_embeddings = reducer.fit_transform(topic_embeddings)\n",
        "\n",
        "        # --- 3. Prepare data\n",
        "        labels = []\n",
        "        for topic_id in topic_ids:\n",
        "            words = self.topic_words_.get(topic_id, [])\n",
        "            top_words = \", \".join(words[:top_n_words])\n",
        "            labels.append(top_words)\n",
        "\n",
        "        df_topics = pd.DataFrame({\n",
        "            \"topic\": topic_ids,\n",
        "            \"Dim_1\": topic_embeddings[:, 0],\n",
        "            \"Dim_2\": topic_embeddings[:, 1],\n",
        "            \"top_words\": labels,\n",
        "            \"size\":topic_sizes,\n",
        "        })\n",
        "\n",
        "        # --- 4. Create chart\n",
        "        chart = (\n",
        "            alt.Chart(df_topics)\n",
        "            .mark_circle(opacity=0.7)\n",
        "            .encode(\n",
        "                x=alt.X(\"Dim_1\", title=\"Topic Dim 1\", scale=alt.Scale(zero=False)),\n",
        "                y=alt.Y(\"Dim_2\", title=\"Topic Dim 2\", scale=alt.Scale(zero=False)),\n",
        "                color=\"topic:N\",\n",
        "                size=alt.Size(\"size:Q\",\n",
        "                              title=\"Documents per Topic\",\n",
        "                              scale=alt.Scale(range=[100, 2000])),  # adjust bubble size\n",
        "                tooltip=[\"topic\", \"top_words\", \"size\"],\n",
        "            )\n",
        "            .interactive()\n",
        "            .properties(width=600, height=500, title=\"Topic Map\")\n",
        "        )\n",
        "\n",
        "        text = (\n",
        "            alt.Chart(df_topics)\n",
        "            .mark_text(align=\"center\", baseline=\"middle\", dy=-15, fontSize=11)\n",
        "            .encode(x=\"Dim_1\", y=\"Dim_2\", text=\"topic\")\n",
        "        )\n",
        "\n",
        "        return chart + text\n",
        "\n",
        "    # =======================================================\n",
        "    #  BONUS!\n",
        "    # =======================================================\n",
        "\n",
        "    def find_similar_documents(self, query: str, top_n: int = 5):\n",
        "        if self.embeddings_ is None or self.documents_ is None:\n",
        "            raise ValueError(\"Model must be fitted before searching for similar documents.\")\n",
        "\n",
        "        # 1. Embed query\n",
        "        query_embedding = self._embed([query])[0]\n",
        "\n",
        "        # 2. Compute cosine similarity\n",
        "        doc_embeddings = self.embeddings_\n",
        "        similarities = np.dot(doc_embeddings, query_embedding) / (\n",
        "            np.linalg.norm(doc_embeddings, axis=1) * np.linalg.norm(query_embedding)\n",
        "        )\n",
        "\n",
        "        # 3. Get top n results\n",
        "        top_indices = np.argsort(similarities)[::-1][:top_n]\n",
        "\n",
        "        results = pd.DataFrame({\n",
        "            \"document\": np.array(self.documents_)[top_indices],\n",
        "            \"similarity\": similarities[top_indices],\n",
        "            \"topic\": np.array(self.topics_)[top_indices],\n",
        "        }).reset_index(drop=True)\n",
        "\n",
        "        return results\n"
      ],
      "metadata": {
        "id": "-aCTO33K3bOQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XrB4EXOJ1URC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we encapsulated and serialized all the different components into a `MyBERTopic` class, we can really simply run topic modeling on any text corpora in just two lines of code! ðŸ’¨\n",
        "\n",
        "```python\n",
        "# Instantiate the topic model\n",
        "topic_model = MyBERTopic(verbose=True)\n",
        "# Run topic modeling on a list of documents\n",
        "topics = topic_model.fit_transform(documents)\n",
        "```\n",
        "\n",
        "Note: you can find this class implemented in a python script: [`mybertopic.py` stored on the GitHub repository](https://github.com/d-noe/NLP_DH_PSL_Fall2025/blob/main/code/scripts/mybertopic.py), you can use it to load this `class` in any script you want with:\n",
        "\n",
        "\n",
        "```python\n",
        "import os\n",
        "os.path.append(\"<PATH_TO_SCRIPT\">) # coudl be helpful\n",
        "from mybertopic import MyBERTopic\n",
        "\n",
        "topic_model = MyBERTopic(verbose=True)\n",
        "# Do your things!\n",
        "```"
      ],
      "metadata": {
        "id": "Wpeha_0ZslPJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ¥˜ Explore 19th Century American Recipes!\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img alt=\"Culture and Cooking; Or, Art in the Kitchen by Catherine Owen\" src=\"https://www.gutenberg.org/cache/epub/29982/pg29982.cover.medium.jpg\" width=\"40%\">\n",
        "&nbsp; &nbsp; &nbsp; &nbsp;\n",
        "  <img alt=\"Directions for Cookery, in its Various Branches by Eliza Leslie\" src=\"https://www.gutenberg.org/cache/epub/9624/images/cover.jpg\" width=\"45%\">\n",
        "</p>"
      ],
      "metadata": {
        "id": "gmtyIvgISlx4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data\n",
        "recipes_df = load_csv_from_github(\"data/topic_data/nineteenth_recipes.csv\")\n",
        "\n",
        "# let's use the full list of recipes now!\n",
        "docs = recipes_df[\"text\"].tolist()"
      ],
      "metadata": {
        "id": "wgkpCsN7r4Jo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "my_topic_model = MyBERTopic(verbose=True)\n",
        "topics = my_topic_model.fit_transform(docs)\n"
      ],
      "metadata": {
        "id": "Dc0WienX8JOz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# /!\\ We visualize the first 2 dimensions but our BERTopic actually uses 5 dimensions for representation and clustering /!\\\n",
        "chart_embeddings = my_topic_model.visualise_embeddings()\n",
        "\n",
        "chart_embeddings"
      ],
      "metadata": {
        "id": "ippaV1IwnqjB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chart_embeddings_no_noise = my_topic_model.visualise_embeddings(\n",
        "    filter_out_noise=True, # remove the \"noise\" cluster\n",
        "    metadata={\"src\":recipes_df[\"ids\"]}, # add additional information on hover\n",
        ")\n",
        "\n",
        "chart_embeddings_no_noise"
      ],
      "metadata": {
        "id": "TMA_KotEUmb8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chart_topics_from_reduced = my_topic_model.visualize_topics(\n",
        "    from_reduced_embeddings=True, # use directly the pre-computed reduced embeddings mean position\n",
        ")\n",
        "\n",
        "chart_topics_from_reduced"
      ],
      "metadata": {
        "id": "QaV6EOoZ_cwH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chart_topics = my_topic_model.visualize_topics(\n",
        "    from_reduced_embeddings=False, # Compute means in high-dimensional embeddings and then perform dimensionality reductions on means\n",
        ")\n",
        "\n",
        "chart_topics"
      ],
      "metadata": {
        "id": "d91Rbsx_SJwV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fbbOt_O4_eqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chart_topic_words = my_topic_model.visualize_barchart()\n",
        "\n",
        "chart_topic_words"
      ],
      "metadata": {
        "id": "Rxfly7oITRv-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7pbk-ul9J8fb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-FtN_JynJ8mB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bonus!"
      ],
      "metadata": {
        "id": "BxAXiAf2UKmk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Retrieval\n",
        "\n",
        "We can use the pre-computed embeddings of the documents to perform retrieval with any text query.\n",
        "\n",
        "Example: Suppose I don't want animals in my plate, can I retrieve vegan recipes with a simple query?"
      ],
      "metadata": {
        "id": "4p1BTDl5UM5g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vegan_recipe_query = \"I would like to find a vegan recipe that includes only vegetables and plant-based ingredients.\"\n",
        "\n",
        "vegan_recipe_similar_docs = my_topic_model.find_similar_documents(\n",
        "    query = vegan_recipe_query\n",
        ")\n",
        "\n",
        "for _, row in vegan_recipe_similar_docs.iterrows():\n",
        "  print(f\"=========== Similarity to query: {row['similarity']} (from topic: {row['topic']})\")\n",
        "  print(row[\"document\"])\n",
        "  print(\"\\n\\n========================================================================\")"
      ],
      "metadata": {
        "id": "TRk0Uwk8otvg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tpdxeUQexdIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3B1IDXOgotlX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LK7eju-eUmMV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-ZkvDgCJ8Je7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}